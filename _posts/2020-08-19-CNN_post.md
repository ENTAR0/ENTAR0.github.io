---
title: CNN(Convolutional Neural Network) 정리
categories:
- Deep learning
last_modified_at: 2020-09-18T14:00:00+09:00
tags:
- Deep learning
- Machine learning
- Image Processing

toc: true
---
# 1. Intro
합성곱 신경망(Convolutional Neural Network)을 처음 알게되었을때 합성곱 신경망은 인간이 물체를 식별하는 방식을 모방한 것일까? 아니면 그냥 블랙박스일까?, 학습된 필터는 무엇을 보고있는것일까?, 왜 합성곱 신경망은 전결합층과는 다르게 위치 의존도(reliance of position)에 무관하단것일까? 등등 여러 궁금증이 떠올랐다. 그리고 여러 자료를 찾아본 결과 그 궁금증을 많이 해소 해주는 강의를 찾아 그 강의를 바탕으로 합성곱 신경망에 대해 정리해보고자 작성하였습니다.  
[Convolutional Neural Networks | MIT 6.S191](https://youtu.be/iaSUYvmCekI)와
[ Understanding Sub-Sampling Layers Within Deep Learning](https://towardsdatascience.com/you-should-understand-sub-sampling-layers-within-deep-learning-b51016acd551), [CNN 개요](https://blog.naver.com/laonple/220587920012), [시각정보 처리과정](https://aidalab.tistory.com/23)을 참고하였습니다.
# 2. Why CNN?
이미지 처리에 있어서 기존에 있는 전결합층(Fully-Connected-Layer)방식을 사용하지않고 합성곱 신경망 방식을 사용하는것은 합성곱 신경망을 사용함으로써 얻을 수 있는 이득이 상당하기 때문입니다.
<p align="center"><img src="https://user-images.githubusercontent.com/56510688/90860659-5efe2d80-e3c5-11ea-8a11-9d2575274a9c.png"></p>
<p align="center">출처:<a href = "https://blog.naver.com/laonple/220587920012">&lt;CNN 개요&gt;</a></p>
왜냐하면 위에 보이는 이미지 처럼 전결합층의 경우에는 픽셀 하나 하나 뉴런의 입력데이터로 받아 처리합니다. 이 경우 모델은 이미지의 Raw data를 학습하기 때문에 위치(Position) 정보에 민감해질 수 밖에 없습니다. 그래서 이 모델의 입력데이터로 변형된 A의 이미지를 줬을때 A에 대한 정확한 판별을 진행할 수 없습니다. 하지만 합성곱 신경망의 경우에는 A가 포함된 이미지에서 A에 대한 특징(Feature)들을 뽑아내어서 그 특징들을 바탕으로 판별하기 때문에 변형된 A에 대해 더욱 유연한 처리를 할 수 있게 됩니다. 한마디로 모델이 공간 정보(Spatial information)을 가진다는 것입니다. 또한 전결합층의 경우 16*16 이미지를 처리하는데 28326개의 매개 변수(parameters)가 필요합니다. 하지만 일반적인 학습에는 더 큰 해상도의 이미지가 들어가 매개변수 개수가 급격하게 증가해 학습 처리에 많은 부정적인 영향을 줍니다. 이외에도 많은 이유가 있다. 간단히 살펴보자면 아래와 같은것들이 있습니다.

- Sharing parameters를 사용해 학습 parameter 개수를 줄일 수 있음
- Spatial Information, 국소성(Locality), 불변성(translation invariance)를 가짐
- Sub-sampling(Pooling)을 사용해 불필요한 세부사항 정보를 제거


# 3. What computers see
이러한 합성곱 신경망 모델은 무엇을 보고 있을까? 기본적으로 사람은 왼쪽에 사진을 보고있지만 모델은 오른쪽 이미지처럼 색상이 2차원 행렬로 변환된 값들을 보고 있습니다.
<p align="center"><img src="https://user-images.githubusercontent.com/56510688/91566494-bdf01380-e97e-11ea-9aa2-19884c55cb2f.png"></p>
<p align="center">출처:<a href = "https://towardsdatascience.com/everything-you-ever-wanted-to-know-about-computer-vision-heres-a-look-why-it-s-so-awesome-e8a58dfb641e">&lt;Everything You Ever Wanted To Know About Computer Vision&gt;</a></p>
그리고 이러한 값들을 기반으로 아래의 과정(Pipeline)을 따라 특징 추출(Feature extraction)을 수행합니다.
<p align="center"><img src="https://user-images.githubusercontent.com/56510688/91590175-e6373c80-e995-11ea-9823-613b818b496a.png"></p>
 하지만 이러한 기능을 수행 하면서 객체 간의 스케일이라던지 바라보는 시점, Brightness와 Contrast의 차이, 왜곡등 여러 변수들 때문에 특징 추출을 수행하는데 문제가 생길 수 있습니다.
그래서 여러 변수에 대응하고자 Image augmentation을 적용하기도 하는데 이는 여기서 다루지는 않겠습니다.



# 4. Learning visual features
그러면 어떤 방식을 사용해야 정확한 시각 정보를 컴퓨터에게 학습시킬 수 있을까?
<p align="center"><img src="https://user-images.githubusercontent.com/56510688/92117010-94783180-ee2f-11ea-8383-feafaebbdef9.png"></p>
<p align="center">출처:<a href = "https://www.researchgate.net/figure/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks_fig4_316950618">&lt;Reserach gate&gt;</a></p>
이를 이해하기 위해서는 수용 영역 또는 장소야(Receptive Field) 라고 불리는 개념을 이해 해야합니다. 수용 영역은 위의 이미지 처럼 한 영역에 대해서 반응하고 다음 레이어에 이를 반영하고 다음 레이어에서 특징의 추상 수준을 높여줍니다. 또한 이런 과정을 거치면서 수용 영역이 겹칠 수 도 있는데 이를 통해 국소 영역간의 상관 관계에 대한 정보를 얻을 수 있습니다.

- 국소영역은 서로 겹칠 수 있다.
- (뇌피셜)정방행렬은 행렬식으로 하나의 실수로 표현할 수 있어 이 실수가 다음 레이어에 전달되면서 실수로 표현된 낮은 수준의 추상 수준이 모여 다음 단계의 추상 수준을 찾는 것으로 생각하고 있습니다.

# 5. Feature extraction and convolution
수용 영역의 반응을 통해서 특징을 검출하는것을 구현한 방법이 바로 합성곱(Convolution)입니다.
<p align="curve"><img width="633" src="https://user-images.githubusercontent.com/56510688/93043019-92ae3980-f68b-11ea-991b-8d718a743d76.png"></p>  
<p align="uncurve"><img src="https://user-images.githubusercontent.com/56510688/93043013-904bdf80-f68b-11ea-9f84-c4656d4b4143.png"></p>
예를 들자면 위의 이미지처럼 곡선을 탐지할 수 있는 필터와 쥐이미지에서 곡선에 해당하는 부분에 대해 합성곱을 수행하면 6600정도의 큰값을 내놓게 되고 반면에, 곡선이 없는 부분에 필터로 합성곱을 수행하게 되면 아주 작은값(0에 가까운 값)을 산출하게 됩니다. 이를 통해 해당 국소영역에 찾고자하는 특징이 있는지 없는지에 대해 판별 할 수 있게 됩니다.

- 필터가 찾고있는 특징과 국소영역이 가지고있는 특징이 비슷할 시 큰값을 산출해낸다.


# 6. Convolution neural networks
본격적으로 CNN의 구조를 보면서 객체를 탐지하기 위해 어떤 방식으로 특징을 적절하게 추출하고 이를 가지고 객체가 무엇 인지를 판단하는 과정을 살펴봅시다.
<p align="center"><img src="https://user-images.githubusercontent.com/56510688/92212618-339f3680-eecd-11ea-9b25-bce7b4589148.png"></p>
<p align="center">출처:<a href = "https://kr.mathworks.com/help/deeplearning/ug/introduction-to-convolutional-neural-networks.html">&lt;Learn About Convolutional Neural Networks&gt;</a></p>
일반적으로 CNN은 두가지 부분으로 나뉘어져있습니다. 첫번째는 합성곱, 활성화 함수 그리고 Pooling계층을 포함하는 특징 검출(Feature Extraction)부분과 두번째는 전결합층과 softmax함수로 이뤄진 분류기(Classifier)부분으로 나뉩니다. 특징 검출부분에 대해 자세히 설명해 보자면, 무작위로 초기화된 필터의 매개변수 값으로 입력 이미지에 대해 Slide해 국소 영역에 대한 Feature map(다음 Conv계층의 입력으로 사용됩니다.)을 뽑아냅니다.
그리고 이 Feature map은 Activation function을 거쳐 매개 변수값을 non-linearity하게 바꾸어주고 Activation map(Conv의 깊이에 따른 추상 수준의 특징을 가집니다.)을 출력합니다. 마지막으로 Pooling계층을 거쳐 Down sampling된 이미지를 출력합니다. 위의 과정을 반복한 후 마지막으로 Classifier부분에서 뽑아낸 활성화 맵의 픽셀을 FCL로 모두 연결 시켜 softmax함수를 통해 이 활성화 맵이 어떤 객체인지에 대한 확률을 제공합니다.

- 필터의 갯수는 입력 이미지의 채널 개수와 같다.
- 무작위로 초기화 하지 않으면 필터가 모두 같은 특징을 추출할 수 있다.
- Feture map은 다음 Convlayer의 인풋으로 사용되고 Activation map은 다음 Convlayer의 Filter로 사용된다.

# 7. Non-linearity and pooling
CNN에서 중요한 개념들중에는 비선형성(Non-linearity)과 pooling이 있습니다.
<p align="center"><img src="https://user-images.githubusercontent.com/56510688/92631052-c3007b80-f30b-11ea-9330-edeeb4385308.jpeg"></p>
<p align="center">출처:<a href = "https://cs231n.github.io/neural-networks-1/">&lt;CS231n CNN for visual Recognition&gt;</a></p>
비선형성을 가진 함수가 왜 중요하냐면 분류를 실행할때 녹색점과 붉은색점을 유의미하게 구분할 수 있으려면 비선형성을 띈 함수를 사용해야하기 때문입니다. 만약 선형 함수를 사용하게 된다면 3번째 그림과 유사하게 정확한 분류를 수행하기에는 무리가 생길 수 있습니다. 비선형성을 구현하기 위해 사용하는 활성화 함수는 여러가지가 있지만 CNN에서는 주로 Relu함수를 사용합니다. 왜냐면 다른 함수의 경우에는 이미지의 값을 처리하기에 적절하지 않고 시그모이드의 경우에는 vanishing gradient라는 문제를 가지고있기 때문입니다..
<p align="center"><img src="https://user-images.githubusercontent.com/56510688/92393946-a45b8280-f15b-11ea-8199-32a82d662df3.png"></p>
pooling은 이미지에 대해 불필요한 세부사항들(details)을 제거하는 Down-sampling의 역할을 수행합니다. 이게 무슨말이냐 하면 사과가 사과로 인식되기 위해서 가지는 최소의 스케일을 가지는 특징을 추출하기 위한 과정이라는 말입니다. 만약 불필요하게 이미지의 스케일이 커서 사과만이 가지는 특징이 아닌것을 추출해 분류하게 된다면 오류가 생기겠죠. 또한 이러한 과정을 수행하면서 위치정보를 무시하게 됩니다. 이는 장단점 둘 다 존재하는데 장점은 이미지의 변형에 대해 대응할 수 있는것이고 단점은 특징들 간의 위치 정보가 필요한 객체를 분류하기에는 부적절해진다는 것입니다. 간단하게 얘기하자면 피카소의 그림에서 나타나는 추상적인 얼굴을 가진 인물과 현실 사람을 둘다 사람이라고 인식한다는 말입니다.
<p align="center"><img src="https://user-images.githubusercontent.com/56510688/92628606-0a850880-f308-11ea-8e11-7433ff1b8562.png"></p>
<p align="center">출처:<a href = "https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451">&lt;Reserach gate&gt;</a></p>
Pooling을 수행하는 방법은 일반적으로 이미지에서 명시된것처럼 두가지 방법이 존재합니다.

- Down Sampling:객체가 객체임을 인지할 수 있는 최소단위의 특징을 뽑아내기 위해 불필요한 Detail을 제거해주는 기법이다.
- 활성화 함수(Relu)를 사용함으로써 음수값은 0으로 매핑하고 양수 값은 그대로 가져가 불필요한 특징을 제거 할 수 있다.

# 8. Applications


# 9. End-to-end self Driving cars
